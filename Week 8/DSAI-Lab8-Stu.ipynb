{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week - 8 Lab Session: MPNeuron and Perceptrons"
      ],
      "metadata": {
        "id": "VpB55YZyEdHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) We can write our own MPNeuron class with the required functions\n",
        "## In the case of MPNeuron, we need to see if the summation of the binary input is larger than a threshold (Slides - page 39).\n",
        "1.   We can create an \"activation\" function to check the threshold.\n",
        "2.   We can also create a \"predict\" function that can return the sum of the input to the activation function.\n",
        "\n"
      ],
      "metadata": {
        "id": "v2jnYFouJ7Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MPNeuron:\n",
        "    def __init__(self, threshold):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def activation(self, x):\n",
        "        return 1 if x >= self.threshold else 0 # Slides - page 39\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        total_input = np.sum(inputs) # Slides - page 39\n",
        "        return self.activation(total_input)\n",
        "\n",
        "# Example: AND gate with MP Neuron, therefore the threshold must be 2, see Slide 54 for example.\n",
        "# AND function must give these outputs 0,0->0;0,1->0;1,0->0;1,1->1 and to achieve this, we need to set the threshold to the correct ðœ½ - Slide page 54\n",
        "# The neuron fires (outputs 1) when the weighted sum is greater than or equal to the threshold, ðœ½.\n",
        "# Since the output should be 1 only when both inputs are 1, we set the threshold Î¸=2.\n",
        "mp_neuron = MPNeuron(threshold=2)\n",
        "\n",
        "# Test Cases using AND input that is known to give output of 0,0,0,1 in order.\n",
        "inputs = np.array([\n",
        "    [0, 0], [0, 1], [1, 0], [1, 1]\n",
        "])\n",
        "\n",
        "for inp in inputs:\n",
        "    print(f\"Input: {inp}, Output: {mp_neuron.predict(inp)}\")\n",
        "\n",
        "# Did you see the correct output for the AND gate for given input arrays? It must be 0,0,0,1 in order.\n",
        "\n",
        "## TASK 1\n",
        "# Now change the threshold ðœ½=1 and see what you get. Can you relate the result to slide 54? There are two misclassified points which you can see at the red crosses, where 0 data points are classified in the region of 1s.\n",
        "\n",
        "## TASK 2\n",
        "# Now change the threshold ðœ½=0.5 and see what you get. Did you any changes compared to the Task 1 and why? Mainly due to the fact that ðœ½=0.5 is not enough to move the line beyond (0,1) and (1,0) points as they must also be classified in the region of 0s - Slide 54\n",
        "# In fact, we cannot set the threshold to a fraction like 0.5 or 1.5 mainly because the input is binary for MPNeuron. Slide 54 is a showcase of perceptron not MPNeuron and Perceptrons can have real values at the input and threshold.\n",
        "\n",
        "## TASK 3\n",
        "# Now change the threshold ðœ½=3 and see what you get. This case gets the data point 1 to be classified in the region of 0s, which is also a misclassification.\n",
        "\n",
        "### Can you see that the precise setting of threshold is very important to attain the right classification outcome!\n",
        "### Can you conclude that we can use MPNeuron to classify data points with a hard decision, either 0 or 1!\n",
        "### Pay attention to the difference: Decision making is not a soft decision as in logistic regression -> probability of 0.4 led to 0, probability of 0.8 led to 1."
      ],
      "metadata": {
        "id": "3EkJ_qu0mWY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Create MPNeuron with Excitatory and Inhibitory inputs\n",
        "## Similar to above can you create a new MPNeuron class that also considers the differentiation of input values (Excitatory and Inhibitory inputs) as indicated in Slides page 40?\n"
      ],
      "metadata": {
        "id": "nIS7ajycJypM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please use the same code base as in the above cell and just modify it so that it also considers inhibitory input. You can see the algorithm on Slide page 40 at the right corner in blue.\n",
        "## Use MPNeuron(threshold=2, inhibitory_indices=[]) as function call, where you can change inhibitory_indices=[] to inhibitory_indices=[0] or inhibitory_indices=[1] -- inhibitory_indices=[] indicates no inhibitory input.\n",
        "## Test Cases is the same as in the above cell.\n",
        "## inputs = np.array([\n",
        "##   [0, 0], [0, 1], [1, 0], [1, 1]\n",
        "## ])\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class MPNeuron: # Modify the __init__() and what input it will take?\n",
        "    def __init__():\n",
        "        \"\"\"\n",
        "        :param threshold: Activation threshold\n",
        "        :param inhibitory_indices: Indices of inhibitory inputs (these override the sum rule)\n",
        "        \"\"\"\n",
        "        ## WRITE HERE\n",
        "\n",
        "    def activation(self, x):\n",
        "        \"\"\"Activation function: returns 1 if sum of excitatory inputs >= threshold, else 0\"\"\"\n",
        "        ## WRITE HERE\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        \"\"\"\n",
        "        Predict the output based on excitatory and inhibitory inputs.\n",
        "        If any inhibitory input is 1, the neuron is forced to output 0.\n",
        "        \"\"\"\n",
        "        ## WRITE HERE\n",
        "\n",
        "# Example: AND gate with one inhibitory input at index 0\n",
        "# If I call inhibitory_indices=[], it should understand that no inhibitory input is available.\n",
        "# Assume that input at any index, e.g., index 0 or 1 of any input values of [0, 0], [0, 1], [1, 0], [1, 1],  is inhibitory.\n",
        "mp_neuron = MPNeuron(threshold=2, inhibitory_indices=[0])\n",
        "\n",
        "# Test Cases\n",
        "inputs = np.array([\n",
        "    [0, 0], [0, 1], [1, 0], [1, 1]\n",
        "])\n",
        "\n",
        "for inp in inputs:\n",
        "    print(f\"Input: {inp}, Output: {mp_neuron.predict(inp)}\")"
      ],
      "metadata": {
        "id": "Qk46o3BVtj1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) We can write our own Perceptron class with the required functions (**Please see Slides 51-63**)\n",
        "## See the differences from MPNeuron in Slide 52.\n",
        "### In summary, input is real number, have weights for different input, threshold (bias) can be learned automatically. No more manual setting of it to integer values as in MPNeuron. In this simple example, we still rely on AND/OR gates as binary input but later we also have real-valued classification example."
      ],
      "metadata": {
        "id": "bAsoY6MXQITN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbabTCfqAtQ7"
      },
      "outputs": [],
      "source": [
        "# Work on this cell and read each comment carefully, and aim for understanding of perceptron using Python alongside the Perceptron slides.\n",
        "# Along with activation and predict functions, we also have a train function, which will update the weight and bias (theta threshold) values to attain the expected outcome, for example AND(1,0)-->0, AND(1,1)-->1.\n",
        "## Basically, to correctly classify the data point of interests within the corresponding regions, perceptron updated the weight and threshold values given the input values so as to get the right output.\n",
        "### To make the correct decision/prediction/classification, it had to learn the parameters by using an update procedure.\n",
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, lr=0.1, epochs=10):\n",
        "        self.weights = np.zeros(input_size + 1)  # Including bias\n",
        "        self.lr = lr # lr is the learning rate, see the slide 63\n",
        "        self.epochs = epochs # Number of iterations required for training so that it converges to the expected outcome along with the optimal values of weights/threshold.\n",
        "\n",
        "    def activation(self, x):\n",
        "        return 1 if x >= 0 else 0 # Step function\n",
        "\n",
        "    def predict(self, inputs): # dot product of the input and weight to be added with bias (weight[0])\n",
        "        return self.activation(np.dot(inputs, self.weights[1:]) + self.weights[0])\n",
        "\n",
        "    def train(self, X, y):\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(X.shape[0]):\n",
        "                prediction = self.predict(X[i])\n",
        "                error = y[i] - prediction\n",
        "                # Update procedure/parameter learning (weight and bias/threshold theta)\n",
        "                # If you wish, you can print out each update on weights and bias/threshold to see the changes\n",
        "                self.weights[1:] += self.lr * error * X[i]\n",
        "                # print(self.weights[1:]) # We have 4 input and 10 epoch each, that brings us to 40 updates.\n",
        "                self.weights[0] += self.lr * error  # Bias update\n",
        "        #print(self.weights)\n",
        "\n",
        "# Example: OR Gate Training, we need to provide the input and the expected ground truth/output.\n",
        "X_or = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_or = np.array([0, 1, 1, 1])  # OR gate labels\n",
        "# We have to do the training for the given input X and output y.\n",
        "perceptron_or = Perceptron(input_size=2)\n",
        "perceptron_or.train(X_or, y_or)\n",
        "\n",
        "\n",
        "# Testing for OR gate: once we develop the perceptron model which learnt the weight and bias and we can start testing using the model.\n",
        "for x in X_or:\n",
        "    print(f\"Input_OR: {x}, Output_OR: {perceptron_or.predict(x)}\")\n",
        "\n",
        "# Example: AND Gate Training, we need to provide the input and the expected ground truth/output.\n",
        "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y_and = np.array([0, 0, 0, 1])  # OR gate labels\n",
        "\n",
        "perceptron_and = Perceptron(input_size=2)\n",
        "perceptron_and.train(X_and, y_and)\n",
        "\n",
        "# Testing for AND gate: Final acquired weight and bias values are used to make the prediction outcome.\n",
        "for x in X_and:\n",
        "    print(f\"Input_AND: {x}, Output_AND: {perceptron_and.predict(x)}\")\n",
        "\n",
        "# If you want to see why training is important, you can remove training member function and from lines wherever it was called. You will see all results are 1s and incorrect outcome."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Use perceptron for classification problem with a real-world data (important for learning curve)"
      ],
      "metadata": {
        "id": "Sc3xbPkLgpho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset using load_breast_cancer() function from sklearn.datasets.\n",
        "\n",
        "# Select first two features for visualization, mean texture/mean radius and allocate to X.\n",
        "\n",
        "# Binary classification labels allocated to y\n",
        "\n",
        "\n",
        "# Split dataset into X_train, X_test, y_train, y_test using train_test_split\n",
        "\n",
        "# Normalize data for better convergence using StandardScaler() function.\n",
        "\n",
        "\n",
        "## The class of a new Perceptron is available for you to use.\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, epochs=50):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.threshold = 0  # Implicit threshold in activation function\n",
        "\n",
        "    def activation(self, x):\n",
        "        return 1 if x >= self.threshold else 0  # Step function\n",
        "\n",
        "    def fit(self, X, y): # training\n",
        "        num_features = X.shape[1]\n",
        "        self.weights = np.zeros(num_features)  # Initialize weights to zero\n",
        "        self.bias = 0  # Initialize bias\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(len(X)):\n",
        "                weighted_sum = np.dot(X[i], self.weights) + self.bias\n",
        "                prediction = self.activation(weighted_sum)\n",
        "\n",
        "                # Weight & Bias Update Rule\n",
        "                error = y[i] - prediction\n",
        "                self.weights += self.learning_rate * error * X[i]\n",
        "                self.bias += self.learning_rate * error\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self.activation(np.dot(x, self.weights) + self.bias) for x in X])\n",
        "\n",
        "# Initialize and train perceptron using learning rate 0.01 and epoch=50.\n",
        "\n",
        "\n",
        "# import accuracy_scores\n",
        "\n",
        "# Predict on test set, X_test\n",
        "\n",
        "# Compute accuracy using accuracy_score\n",
        "\n",
        "\n",
        "# Show final learned weights and bias\n",
        "\n",
        "\n",
        "# Plotting decision boundary function is created for you for visualisation of the decision boundary. Please keep X, y, perceptron, split variable names as is. X is capital as it is an array.\n",
        "def plot_decision_boundary(X, y, perceptron):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
        "\n",
        "    Z = np.array([perceptron.predict(np.array([[x, y]]))[0] for x, y in zip(xx.ravel(), yy.ravel())])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=\"coolwarm\")\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=\"coolwarm\")\n",
        "    plt.title(\"Perceptron Decision Boundary for Cancer Classification\")\n",
        "    plt.xlabel(data.feature_names[0])\n",
        "    plt.ylabel(data.feature_names[1])\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(X_test, y_test, perceptron)"
      ],
      "metadata": {
        "id": "Jz6LBnEG6RUF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}