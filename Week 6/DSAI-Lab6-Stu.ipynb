{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A50WnOoUZMrX"
      },
      "source": [
        "# Week 6: Laboratory work on Clustering with K-means and DBSCAN and evaluating via Silhouette Coefficient\n",
        "   * Remember what K-means and DBSCAN were and how they work. You can have a look at the slides or online materials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfYLd2gcvRrO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSJobYxHZwLM"
      },
      "source": [
        "# Learning objectives:\n",
        "Understand and implement K-Means clustering.\n",
        "\n",
        "1.   Understand and implement K-Means clustering.\n",
        "2.   Understand and implement DBSCAN clustering.\n",
        "3.   Evaluate both models using the Silhouette Coefficient.\n",
        "4.   Compare the results and discuss the strengths and weaknesses of each method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGxJjQs4d02Z"
      },
      "source": [
        "# 1- Load and Preprocess Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mDvodGR7ZJAY"
      },
      "outputs": [],
      "source": [
        "# Import libraries, numpy, pandas, load_iris, StandardScaler\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PS5pnvNbaVsJ"
      },
      "outputs": [],
      "source": [
        "# Please load Iris dataset to 'data' variable \"from sklearn.datasets import load_iris\"\n",
        "\n",
        "data = load_iris() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhhEfi8acCat"
      },
      "outputs": [],
      "source": [
        "# Standardize the data\n",
        "# Standardization ensures all features have a mean of 0 and a standard deviation of 1, which is crucial for distance-based clustering methods like K-Means and DBSCAN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGe8cMREdkjU"
      },
      "outputs": [],
      "source": [
        "# Display the first few rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDDHuscWd9RH"
      },
      "source": [
        "# 2- K-means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhTbcZSOeCXf"
      },
      "outputs": [],
      "source": [
        "# Import Kmeans and silhouette_score from sklearn - search for it online\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV2qlckseq4C"
      },
      "outputs": [],
      "source": [
        "# Apply K-Means clustering using KMeans() and fit_predict()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAJ94YChe14f"
      },
      "outputs": [],
      "source": [
        "# Evaluate the clustering performance\n",
        "# Change the number of clusters to (2, 4, 5) and observe how the change affect Silhouette Score.\n",
        "# Discuss your observations with another student next to you.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kshVYrArgR7t"
      },
      "source": [
        "# Helper for discussion:\n",
        "*The Silhouette Score peaks when the number of clusters matches the natural structure in the data.*\n",
        "\n",
        "Changing **k** affects both cohesion and separation:\n",
        "\n",
        "*   Too few clusters: High overlap (poor cohesion).\n",
        "\n",
        "*   Optimal clusters: Balance of cohesion and separation.\n",
        "\n",
        "*   Too many clusters: Over-splitting reduces separation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5PZPYxCg1Ae"
      },
      "source": [
        "# 3- DBSCAN Clustering\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfgKI1Esmja1"
      },
      "source": [
        "DBSCAN is a density-based clustering algorithm, and its behavior is significantly influenced by two parameters:\n",
        "\n",
        "*   eps (epsilon): The maximum distance between two points to consider them part of the same cluster.\n",
        "\n",
        "*   min_samples: The minimum number of points required to form a dense region (i.e., a cluster).\n",
        "\n",
        "When you experiment with these parameters, the number and quality of clusters, as measured by the Silhouette Score, will change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF5mrd6FiPqY"
      },
      "outputs": [],
      "source": [
        "# Import DBSCAN from sklearn - search online for this\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciNftGzCiZSe"
      },
      "outputs": [],
      "source": [
        "# Apply DBSCAN clustering with default parameters (eps=0.5, min_samples=5). Use DBSCAN() and fit_predict() functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc8TsZZhiuK3"
      },
      "outputs": [],
      "source": [
        "# Filter out noise points from 'filtered_labels' and 'filtered_data' for Silhouette Score calculation\n",
        "filtered_labels =\n",
        "filtered_data ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a8xKJBgi2qk"
      },
      "outputs": [],
      "source": [
        "# Evaluate DBSCAN clustering\n",
        "# Make sure that Silhouette Score requires at least 2 clusters so the length of 'filtered_labels' need to be larger than 1.\n",
        "# Experiment with different eps values, e.g., 0.3, 0.7 and min_samples, e.g., 3, 10 to observe how the clusters and Silhouette Scores change.\n",
        "if len(set(filtered_labels)) > 1:  # Silhouette Score requires at least 2 clusters\n",
        "\n",
        "    #### Your one line code is HERE\n",
        "\n",
        "    print(f\"DBSCAN Silhouette Score: {dbscan_silhouette}\")\n",
        "else:\n",
        "    print(\"DBSCAN could not form enough clusters for Silhouette evaluation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsJxguIOmBIZ"
      },
      "source": [
        "## 3- Visualise K-means and DBSCAN Clusters\n",
        "*   Visualize the clusters formed by K-Means and DBSCAN using the first two features (*sepal length* and *sepal width*)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LWXlkXtnNcR"
      },
      "outputs": [],
      "source": [
        "# import matplotlib as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOYijXIynYVn"
      },
      "outputs": [],
      "source": [
        "# K-Means Visualization\n",
        "# Use Scatterplot\n",
        "\n",
        "#### Write your code HERE\n",
        "\n",
        "#---------------------------------------------\n",
        "# DBSCAN Visualization\n",
        "# Use Scatterplot\n",
        "\n",
        "#### Write your code HERE\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GiOe-DTn2XY"
      },
      "source": [
        "# 4- Discussions\n",
        "  1. Compare the Silhouette Scores of K-Means and DBSCAN. Which algorithm performed better and why?\n",
        "      * Feature importance distribution: Scaling gives equal weight to less informative features, confusing K-Means. Silhouette Scores are influenced by the compactness and separation of clusters. When scaled data introduces overlap between clusters, K-Means may have poorly separated clusters, resulting in lower scores.\n",
        "      * Cluster overlap: DBSCAN handles overlapping clusters better than K-Means.\n",
        "      * Noise handling: DBSCAN can classify ambiguous points as noise, while K-Means forces them into clusters, distorting results.\n",
        "      * Assumptions: DBSCAN doesn't assume spherical clusters, which gives it an advantage for datasets like Iris.\n",
        "\n",
        "  2. Discuss the strengths and weaknesses of both algorithms.\n",
        "      * K-Means is a simple, efficient, and scalable algorithm, but it assumes spherical clusters and struggles with noise.\n",
        "      * DBSCAN excels in identifying arbitrary-shaped clusters and handling noise, but it requires careful parameter tuning and may struggle with varying cluster densities.\n",
        "  3. Discuss scenarios where one algorithm might outperform the other.\n",
        "    * Scenarios Where K-Means Outperforms DBSCAN:\n",
        "      * Clusters are spherical, well-separated, and of similar size.\n",
        "      * Large datasets with no significant noise or outliers.\n",
        "      * The number of clusters is known in advance.\n",
        "    * Scenarios Where DBSCAN Outperforms K-Means:\n",
        "      * Clusters are irregularly shaped or vary in size.\n",
        "      * The dataset has significant noise or outliers.\n",
        "      * The number of clusters is unknown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoXs1SDWu0D5"
      },
      "source": [
        "# 5 - Deep understanding of parameters in DBSCAN\n",
        "## Task-1: Run the following code cell and observe the output with default values - eps_value=1 and min_samples_value=10\n",
        "\n",
        "## Task-2: Change eps_value from 1 to 5 while keeping min_samples_value=10, and observe the changes in the results and comment/discuss.\n",
        "\n",
        "## Task-3: Keep eps_value=1 while changing min_samples_value from 10 to 100, and observe the changes in the results and comment/discuss.\n",
        "\n",
        "## Task-4: Keep eps_value=1 while changing min_samples_value from 10 to (60,70,80), and observe the changes in the results and what was the final value you could see clustering without having all treated as noise?\n",
        "\n",
        "## Task-5: Change min_samples_value to 2 while keeping eps_value=1, and observe the changes in the results and comment/discuss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYKKTqbAu8RJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Generate synthetic dataset\n",
        "n_samples = 1000  # More data points for better visualization\n",
        "centers = [[2, 2], [8, 8], [5, 2], [2, 8]]  # Cluster centers\n",
        "X, _ = make_blobs(n_samples=n_samples, centers=centers, cluster_std=1.2, random_state=42)\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "eps_value = 1  # Defines the max distance for neighbors\n",
        "min_samples_value = 10  # Minimum points required to form a dense region\n",
        "db = DBSCAN(eps=eps_value, min_samples=min_samples_value)\n",
        "labels = db.fit_predict(X)\n",
        "\n",
        "# Identify core, border, and noise points\n",
        "core_samples_mask = np.zeros_like(labels, dtype=bool)\n",
        "core_samples_mask[db.core_sample_indices_] = True  # Mark core points\n",
        "unique_labels = set(labels)\n",
        "\n",
        "# Define colors and markers for visualization\n",
        "color_map = {0: \"blue\", 1: \"green\", 2: \"purple\", 3: \"orange\"}\n",
        "marker_map = {0: \"o\", 1: \"s\", 2: \"^\", 3: \"P\"}\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for label in unique_labels:\n",
        "    if label == -1:\n",
        "        # Noise points (outliers)\n",
        "        color, marker, label_name = 'red', 'x', \"Noise\"\n",
        "    else:\n",
        "        # Assign colors and markers to clusters dynamically\n",
        "        color = color_map.get(label, \"gray\")\n",
        "        marker = marker_map.get(label, \"D\")\n",
        "        label_name = f\"Cluster {label}\"\n",
        "\n",
        "    # Select points in this cluster\n",
        "    class_member_mask = labels == label\n",
        "\n",
        "    # Core points (big solid markers)\n",
        "    xy = X[class_member_mask & core_samples_mask]\n",
        "    plt.scatter(xy[:, 0], xy[:, 1], s=80, c=color, marker=marker, label=f\"{label_name} (Core)\", edgecolors='k')\n",
        "\n",
        "    # Border points (smaller semi-transparent markers)\n",
        "    xy = X[class_member_mask & ~core_samples_mask]\n",
        "    plt.scatter(xy[:, 0], xy[:, 1], s=50, c=color, marker=marker, label=f\"{label_name} (Border)\", alpha=0.6, edgecolors='r')\n",
        "\n",
        "# Plot DBSCAN parameters\n",
        "plt.title(f\"DBSCAN Clustering (eps={eps_value}, min_samples={min_samples_value})\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend(loc=\"upper right\", fontsize=10)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
