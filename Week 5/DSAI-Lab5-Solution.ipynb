{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d6276cad-2b6a-4378-af30-cd0ca644de3a",
      "metadata": {
        "id": "d6276cad-2b6a-4378-af30-cd0ca644de3a"
      },
      "source": [
        "## Week-5 Lab - Logistic Regression and Random Forest Model Development (Classification)\n",
        "1) We will create a logistic regression model that will predict whether or not a user will click on an ad, based on the given features. As this is a binary classification problem, a logistic regression model is well suited here.\n",
        "\n",
        "2) There is also a more challenging approach to this problem using random forest.\n",
        "\n",
        "**Details to be found in the following cells.**\n",
        "\n",
        "**You are expected to create new cells as much as you think you need to.**\n",
        "\n",
        "Dataset is available at: https://www.kaggle.com/datasets/debdyutidas/advertisingcsv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xYmkTXsXuLPR",
      "metadata": {
        "id": "xYmkTXsXuLPR"
      },
      "source": [
        "# 1- Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aEppjx6Ctjh",
      "metadata": {
        "id": "3aEppjx6Ctjh"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0781872d-aa48-463e-a86d-35d79a54e6d0",
      "metadata": {
        "id": "0781872d-aa48-463e-a86d-35d79a54e6d0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Daily Time Spent on Site</th>\n",
              "      <th>Age</th>\n",
              "      <th>Area Income</th>\n",
              "      <th>Daily Internet Usage</th>\n",
              "      <th>Ad Topic Line</th>\n",
              "      <th>City</th>\n",
              "      <th>Male</th>\n",
              "      <th>Country</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Clicked on Ad</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>68.95</td>\n",
              "      <td>35</td>\n",
              "      <td>61833.90</td>\n",
              "      <td>256.09</td>\n",
              "      <td>Cloned 5thgeneration orchestration</td>\n",
              "      <td>Wrightburgh</td>\n",
              "      <td>0</td>\n",
              "      <td>Tunisia</td>\n",
              "      <td>2016-03-27 00:53:11</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>80.23</td>\n",
              "      <td>31</td>\n",
              "      <td>68441.85</td>\n",
              "      <td>193.77</td>\n",
              "      <td>Monitored national standardization</td>\n",
              "      <td>West Jodi</td>\n",
              "      <td>1</td>\n",
              "      <td>Nauru</td>\n",
              "      <td>2016-04-04 01:39:02</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>69.47</td>\n",
              "      <td>26</td>\n",
              "      <td>59785.94</td>\n",
              "      <td>236.50</td>\n",
              "      <td>Organic bottom-line service-desk</td>\n",
              "      <td>Davidton</td>\n",
              "      <td>0</td>\n",
              "      <td>San Marino</td>\n",
              "      <td>2016-03-13 20:35:42</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>74.15</td>\n",
              "      <td>29</td>\n",
              "      <td>54806.18</td>\n",
              "      <td>245.89</td>\n",
              "      <td>Triple-buffered reciprocal time-frame</td>\n",
              "      <td>West Terrifurt</td>\n",
              "      <td>1</td>\n",
              "      <td>Italy</td>\n",
              "      <td>2016-01-10 02:31:19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>68.37</td>\n",
              "      <td>35</td>\n",
              "      <td>73889.99</td>\n",
              "      <td>225.58</td>\n",
              "      <td>Robust logistical utilization</td>\n",
              "      <td>South Manuel</td>\n",
              "      <td>0</td>\n",
              "      <td>Iceland</td>\n",
              "      <td>2016-06-03 03:36:18</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>72.97</td>\n",
              "      <td>30</td>\n",
              "      <td>71384.57</td>\n",
              "      <td>208.58</td>\n",
              "      <td>Fundamental modular algorithm</td>\n",
              "      <td>Duffystad</td>\n",
              "      <td>1</td>\n",
              "      <td>Lebanon</td>\n",
              "      <td>2016-02-11 21:49:00</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>51.30</td>\n",
              "      <td>45</td>\n",
              "      <td>67782.17</td>\n",
              "      <td>134.42</td>\n",
              "      <td>Grass-roots cohesive monitoring</td>\n",
              "      <td>New Darlene</td>\n",
              "      <td>1</td>\n",
              "      <td>Bosnia and Herzegovina</td>\n",
              "      <td>2016-04-22 02:07:01</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>51.63</td>\n",
              "      <td>51</td>\n",
              "      <td>42415.72</td>\n",
              "      <td>120.37</td>\n",
              "      <td>Expanded intangible solution</td>\n",
              "      <td>South Jessica</td>\n",
              "      <td>1</td>\n",
              "      <td>Mongolia</td>\n",
              "      <td>2016-02-01 17:24:57</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>55.55</td>\n",
              "      <td>19</td>\n",
              "      <td>41920.79</td>\n",
              "      <td>187.95</td>\n",
              "      <td>Proactive bandwidth-monitored policy</td>\n",
              "      <td>West Steven</td>\n",
              "      <td>0</td>\n",
              "      <td>Guatemala</td>\n",
              "      <td>2016-03-24 02:35:54</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>45.01</td>\n",
              "      <td>26</td>\n",
              "      <td>29875.80</td>\n",
              "      <td>178.35</td>\n",
              "      <td>Virtual 5thgeneration emulation</td>\n",
              "      <td>Ronniemouth</td>\n",
              "      <td>0</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>2016-06-03 21:43:21</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Daily Time Spent on Site  Age  Area Income  Daily Internet Usage  \\\n",
              "0                       68.95   35     61833.90                256.09   \n",
              "1                       80.23   31     68441.85                193.77   \n",
              "2                       69.47   26     59785.94                236.50   \n",
              "3                       74.15   29     54806.18                245.89   \n",
              "4                       68.37   35     73889.99                225.58   \n",
              "..                        ...  ...          ...                   ...   \n",
              "995                     72.97   30     71384.57                208.58   \n",
              "996                     51.30   45     67782.17                134.42   \n",
              "997                     51.63   51     42415.72                120.37   \n",
              "998                     55.55   19     41920.79                187.95   \n",
              "999                     45.01   26     29875.80                178.35   \n",
              "\n",
              "                             Ad Topic Line            City  Male  \\\n",
              "0       Cloned 5thgeneration orchestration     Wrightburgh     0   \n",
              "1       Monitored national standardization       West Jodi     1   \n",
              "2         Organic bottom-line service-desk        Davidton     0   \n",
              "3    Triple-buffered reciprocal time-frame  West Terrifurt     1   \n",
              "4            Robust logistical utilization    South Manuel     0   \n",
              "..                                     ...             ...   ...   \n",
              "995          Fundamental modular algorithm       Duffystad     1   \n",
              "996        Grass-roots cohesive monitoring     New Darlene     1   \n",
              "997           Expanded intangible solution   South Jessica     1   \n",
              "998   Proactive bandwidth-monitored policy     West Steven     0   \n",
              "999        Virtual 5thgeneration emulation     Ronniemouth     0   \n",
              "\n",
              "                    Country            Timestamp  Clicked on Ad  \n",
              "0                   Tunisia  2016-03-27 00:53:11              0  \n",
              "1                     Nauru  2016-04-04 01:39:02              0  \n",
              "2                San Marino  2016-03-13 20:35:42              0  \n",
              "3                     Italy  2016-01-10 02:31:19              0  \n",
              "4                   Iceland  2016-06-03 03:36:18              0  \n",
              "..                      ...                  ...            ...  \n",
              "995                 Lebanon  2016-02-11 21:49:00              1  \n",
              "996  Bosnia and Herzegovina  2016-04-22 02:07:01              1  \n",
              "997                Mongolia  2016-02-01 17:24:57              1  \n",
              "998               Guatemala  2016-03-24 02:35:54              0  \n",
              "999                  Brazil  2016-06-03 21:43:21              1  \n",
              "\n",
              "[1000 rows x 10 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Please conduct exploratory data analysis and preprocessing as required. You can follow the steps in our previous workshops and laboratory works.\n",
        "# Remember that the focus in this session is to build a classification model.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "ad_data = pd.read_csv('datasets/advertising.csv')\n",
        "ad_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61b939ff-66fa-45c6-a9ad-f6dbe265999f",
      "metadata": {
        "id": "61b939ff-66fa-45c6-a9ad-f6dbe265999f"
      },
      "source": [
        "## Model building"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dee7484-c317-4392-8bd1-4262f366d709",
      "metadata": {
        "id": "2dee7484-c317-4392-8bd1-4262f366d709"
      },
      "source": [
        "Let us split the data into training set and testing set using train_test_split, but first, letâ€™s convert\n",
        "the â€˜Countryâ€™ feature to an acceptable form for the model Country is a categorical string and we need to find a way to feed this important piece of information into the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62f9c1d4-2bea-48ee-897a-2eec2fbac275",
      "metadata": {
        "id": "62f9c1d4-2bea-48ee-897a-2eec2fbac275"
      },
      "source": [
        "It is easy to drop this feature but this means we need to sacrifice an important piece of information for the model to perform more realistic.\n",
        "We can convert the categorical feature into [dummy variables](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) using pandas.\n",
        "\n",
        "* We convert categorical features into dummy variables (also called one-hot encoding) because machine learning models work with numerical data and can't directly process categories or labels as inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acafd263-c32d-432a-a3a7-a73c8f143308",
      "metadata": {
        "id": "acafd263-c32d-432a-a3a7-a73c8f143308"
      },
      "outputs": [],
      "source": [
        "# Create countries dummy variable using Country column. Student needs to search about dummy variable.\n",
        "# How to create dummy variables? And why do we need them? Search for simple examples.\n",
        "# Then run the available codes below and understand how the data operations are done.\n",
        "\n",
        "ad_data.columns\n",
        "ad_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c5953e8-1ceb-45b6-b2fb-2d50d6a8ae40",
      "metadata": {
        "id": "1c5953e8-1ceb-45b6-b2fb-2d50d6a8ae40"
      },
      "outputs": [],
      "source": [
        "# All the rows in the Country feature is uniquely converted to new features for the dataset.\n",
        "countries = pd.get_dummies(ad_data['Country'],drop_first=True)\n",
        "print(countries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4468c60d-7971-4a0c-a005-4d417a5c510e",
      "metadata": {
        "id": "4468c60d-7971-4a0c-a005-4d417a5c510e"
      },
      "outputs": [],
      "source": [
        "# Concatenating dummy variables with the original dataset, and dropping other features (repetitive and irrelevant ones).\n",
        "# Country feature can also be removed as we do not need it any more.\n",
        "# With new dummy variables we have already reflected the country feature.\n",
        "ad_data = pd.concat([ad_data,countries],axis=1)\n",
        "ad_data.drop(['Country','Ad Topic Line','City','Timestamp'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acc39204-e71e-48a6-84c0-6cedd38d1d6c",
      "metadata": {
        "id": "acc39204-e71e-48a6-84c0-6cedd38d1d6c"
      },
      "outputs": [],
      "source": [
        "ad_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f6a0639-833c-485d-b6e5-b8fac0c22227",
      "metadata": {
        "id": "2f6a0639-833c-485d-b6e5-b8fac0c22227"
      },
      "outputs": [],
      "source": [
        "# Allocate and assign the variables apropriately and prepare for fitting training data\n",
        "X = ad_data.drop('Clicked on Ad',axis=1) # Everything else exept the Clicked on Ad column\n",
        "y = ad_data['Clicked on Ad']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce1c2238-22d4-4762-81ea-0dfb5587c22b",
      "metadata": {
        "id": "ce1c2238-22d4-4762-81ea-0dfb5587c22b"
      },
      "outputs": [],
      "source": [
        "# # Split the dataset apropriately test being 30% of the dataset.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f832c57-6cd0-43c6-84e0-b41a037316b5",
      "metadata": {
        "id": "4f832c57-6cd0-43c6-84e0-b41a037316b5"
      },
      "outputs": [],
      "source": [
        "# Train the model using logistic regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logclf = LogisticRegression(max_iter=10000)\n",
        "logclf.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05da9e6b-2a6a-40a3-ad0c-e4c3cb843ab3",
      "metadata": {
        "id": "05da9e6b-2a6a-40a3-ad0c-e4c3cb843ab3"
      },
      "source": [
        "## Predictions and Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c94a86c-581f-4bd2-93be-aebe68d15bad",
      "metadata": {
        "id": "8c94a86c-581f-4bd2-93be-aebe68d15bad"
      },
      "outputs": [],
      "source": [
        "# Get the prediction results\n",
        "predictions = logclf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c77592f6-1e51-413e-a9e6-865567809384",
      "metadata": {
        "id": "c77592f6-1e51-413e-a9e6-865567809384"
      },
      "source": [
        "## Classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6864e5d3-95dd-4cc1-9609-df744a2d586d",
      "metadata": {
        "id": "6864e5d3-95dd-4cc1-9609-df744a2d586d"
      },
      "source": [
        "**Precision** and recall are two important metrics used to evaluate the performance of a classification\n",
        "model. Precision measures the proportion of positive predictions that are actually true positive. In\n",
        "other words, it is the ratio of true positive predictions to the total number of positive predictions.\n",
        "A high precision indicates that the model is making accurate positive predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1d1c183-51a4-4335-beff-01eb6d30c918",
      "metadata": {
        "id": "d1d1c183-51a4-4335-beff-01eb6d30c918"
      },
      "source": [
        "**Recall** measures the proportion of actual positive cases that are correctly identified by the model. In other words, it is the ratio of true positive predictions to the total number of actual positive cases. A high recall indicates that the model is effectively identifying positive cases."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8541701e-bc1d-4610-b095-2adea65424e4",
      "metadata": {
        "id": "8541701e-bc1d-4610-b095-2adea65424e4"
      },
      "source": [
        "Simple Analogy:\n",
        "* Precision: If you want to catch all the apples falling from a tree with a basket, precision is about making sure the basket only contains apples and not leaves or stones.\n",
        "* Recall: Recall is about catching as many apples as possible with your basket, even if a few slip through.\n",
        "* Precision focuses on the quality of the positive predictions (fewer false positives).\n",
        "* Recall focuses on how many actual positives were correctly identified (fewer false negatives)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d343143-1771-4954-9c32-786889857e93",
      "metadata": {
        "id": "1d343143-1771-4954-9c32-786889857e93"
      },
      "outputs": [],
      "source": [
        "# How well the prediction is made? Check with classification report.\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c3dc21d-8357-4193-90a3-0eb7c618ff9c",
      "metadata": {
        "id": "2c3dc21d-8357-4193-90a3-0eb7c618ff9c"
      },
      "outputs": [],
      "source": [
        "# Print the confusion matrix for the predictions and actual values.\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm=confusion_matrix(y_test,predictions)\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.title(\"Confusion Matrix\")\n",
        "sns.heatmap(cm, annot=True,fmt='d', cmap='Blues')\n",
        "plt.ylabel(\"Actual Values\")\n",
        "plt.xlabel(\"Predicted Values\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30041684-d0e1-46fb-b1a8-9e0895f27465",
      "metadata": {
        "id": "30041684-d0e1-46fb-b1a8-9e0895f27465"
      },
      "outputs": [],
      "source": [
        "# By means of using the confusion matrix, can you work out the accuracy, precision, recall, F1-Score values for each classes by pen and paper?\n",
        "# Compare your findings in the classification report results. It is important to see which class you are taking as the true class (1 (click-on-ad) or 0 (no-click)) and understand how the precision/recall calculations are affected.\n",
        "# Hint: Look at your slides."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qlLraKS1PodH",
      "metadata": {
        "id": "qlLraKS1PodH"
      },
      "source": [
        "# 2) Random Forest\n",
        "## Challenge - Use the same X and y sets to train a Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cp1Hfls2M8gT",
      "metadata": {
        "id": "Cp1Hfls2M8gT"
      },
      "outputs": [],
      "source": [
        "# Train a model using random forest, a tree-based model, with default parameters.\n",
        "# First, search for how to import random forest classifier from sklearn.\n",
        "# Make sure to use new variable names for the model and the prediction outcomes.\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_RF = rf_model.predict(X_test)\n",
        "print(classification_report(y_test,y_pred_RF))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eQgnu6A-cy0p",
      "metadata": {
        "id": "eQgnu6A-cy0p"
      },
      "source": [
        "## What did you observe from classification report? Is the performance better or worse compared to logistic regression? Any surprises?\n",
        "\n",
        "*   The performance is 2-3 percentage points is lower compared to logistic regression. Is it surprising? We will come to that later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Yo0gzWXhOlCl",
      "metadata": {
        "id": "Yo0gzWXhOlCl"
      },
      "outputs": [],
      "source": [
        "# Import libraries for RandomizedSearchCV, randint, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Tree Visualisation\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import Image\n",
        "import graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IK7yRrEuQ8kn",
      "metadata": {
        "id": "IK7yRrEuQ8kn"
      },
      "outputs": [],
      "source": [
        "# Create the confusion matrix for the above prediction\n",
        "cm_0 = confusion_matrix(y_test, y_pred_RF)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm_0).plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jTBslUXzerQl",
      "metadata": {
        "id": "jTBslUXzerQl"
      },
      "source": [
        "## Can you comment on the confusion matrix based on True Class=1. What is your observations for TP, TN, FN and FP? Compare these findings to logistic regression and comment on your findings.\n",
        "\n",
        "*   Results can change each time you run the random-forest.\n",
        "*   TP= 136, TN=152, FN=7 and FP=5 for the random forest model during this session.\n",
        "\n",
        "## This is actually a good performance. However, we may be able to get a better performance by optimizing our hyperparameters. Let us see if this can actually help for this problem set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1wuxmTGNOmVR",
      "metadata": {
        "id": "1wuxmTGNOmVR"
      },
      "outputs": [],
      "source": [
        "# You can export the first three decision trees from the forest and visualise using export_graphviz.\n",
        "# Please search online and see how you can implement this. Observe your features and how they were represented at the tree-based structure.\n",
        "\n",
        "for i in range(3):\n",
        "    tree = rf_model.estimators_[i]\n",
        "    dot_data = export_graphviz(tree,\n",
        "                               feature_names=X_train.columns,\n",
        "                               filled=True,\n",
        "                               max_depth=2,\n",
        "                               impurity=False,\n",
        "                               proportion=True)\n",
        "    graph = graphviz.Source(dot_data)\n",
        "    display(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5RYex9EUhi70",
      "metadata": {
        "id": "5RYex9EUhi70"
      },
      "source": [
        "# Can you explain what do the boxes and the values on it indicate? Can you relate this to how a decision is made? There will be a discussion on this during the drop-in session, not to be missed!\n",
        "Each tree can only show the first few nodes and they can be very large and infeasible to visualise. The colors represent the majority class of each node (box, with orange indicating majority 0 (no-ad-clicking) and blue indicating majority 1 (ad-clicking). The colors get darker the closer the node gets to being fully 0 or 1. Each node also contains the following information:\n",
        "- The variable name and value used for splitting\n",
        "- The % of total samples in each split\n",
        "- The % split between classes in each split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-HX74bHclxZT",
      "metadata": {
        "id": "-HX74bHclxZT"
      },
      "source": [
        "# Hyperparameter Tuning: Can you find the best hyperparameter values for random forest model using RandomizedSearchCV? Please use n_estimators and max_depth, and decide on a range for these two hyperparameters.\n",
        "\n",
        "## We are using RandomizedSearchCV to search for the best hyperparameter values within a range. We can define the hyperparameters to use and their range in the param_dist dictionary.\n",
        "- n_estimators: the number of decision trees in the forest. Say this is 5, then there will be 5 decision trees created using random features to make a final aggregated decision once a new observation has arrived to the model to click or not to click an ad.\n",
        "- max_depth: the maximum depth of each decision tree in the forest. This indicates how many decision layers you can have per decision tree (n_estimators).\n",
        "- RandomizedSearchCV will train many models (defined by n_iter_ and save each one as variables)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lIJAJ_weQKcD",
      "metadata": {
        "id": "lIJAJ_weQKcD"
      },
      "outputs": [],
      "source": [
        "# Create the param_dist dictionary for the two hyperparameters with a range.\n",
        "param_dist = {'n_estimators': randint(50,500),\n",
        "              'max_depth': randint(1,20)}\n",
        "\n",
        "# Create a new random forest classifier, maybe a name like: rf_model_hp\n",
        "rf_model_hp = RandomForestClassifier()\n",
        "\n",
        "# Use RandomizedSearchCV to find the best hyperparameters\n",
        "rand_search = RandomizedSearchCV(rf_model_hp,\n",
        "                                 param_distributions = param_dist,\n",
        "                                 n_iter=5,\n",
        "                                 cv=5)\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data again.\n",
        "rand_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CVbcszK5Qjm3",
      "metadata": {
        "id": "CVbcszK5Qjm3"
      },
      "outputs": [],
      "source": [
        "# Create a variable for the best model, e.g., best_rf is nice variable name.\n",
        "best_rf = rand_search.best_estimator_\n",
        "\n",
        "# Print the best hyperparameters, you will see that each model run (running above cell) may generate a different hyperparameter value.\n",
        "print('Best hyperparameters:',  rand_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pPE-yv06QvR5",
      "metadata": {
        "id": "pPE-yv06QvR5"
      },
      "outputs": [],
      "source": [
        "# Use the best_rf and generate predictions with the best model\n",
        "y_pred_RF_hp = best_rf.predict(X_test)\n",
        "print(classification_report(y_test,y_pred_RF_hp))\n",
        "# Create the confusion matrix for the improved-hyperparameter model\n",
        "cm = confusion_matrix(y_test, y_pred_RF_hp)\n",
        "ConfusionMatrixDisplay(confusion_matrix=cm).plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UGE3BGI2o2tO",
      "metadata": {
        "id": "UGE3BGI2o2tO"
      },
      "source": [
        "# Share your observations compared to the previous random forest model.\n",
        "- Did you see any improvements in the performance of the model when the best hyperparameter values are applied.\n",
        "- It is expected that different colab sessions will produce different values but highly likely with similar conclusions if not the same! So it is important that you interprete these results on your own and then show your results to a friend next to you and see what they got so that you can discuss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6-Z_FP3eqPjF",
      "metadata": {
        "id": "6-Z_FP3eqPjF"
      },
      "outputs": [],
      "source": [
        "# Finally, can you now create a series containing feature importances from the model and feature names from the training data\n",
        "# Compute feature importances\n",
        "feature_importances = pd.Series(best_rf.feature_importances_, index=X_train.columns)\n",
        "\n",
        "# Set threshold for importance (e.g., features with importance > 0.005)\n",
        "threshold = 0.005\n",
        "important_features = feature_importances[feature_importances > threshold].sort_values(ascending=False)\n",
        "\n",
        "# Plot a simple bar chart for the important features only!\n",
        "important_features.plot.bar();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OmKOGKCsrhCr",
      "metadata": {
        "id": "OmKOGKCsrhCr"
      },
      "source": [
        "# Wrap-up\n",
        "- We have developed a logistic regression model on a simple dataset to predict whether a person will click on an ad (classification problem).\n",
        "- We then developed a random forest model and compared the findings to logistic regression. Random forest performed sligtly worse performance compared to logistic regression\n",
        "  - Normally, our expectation is that random forest should perform better. Why do you think it was not the case? Let us discuss this during the drop-in session.\n",
        "- We visualised decision-trees and practised on finding the best hyperparameter values.\n",
        "  - We still observed that only a slightly improvement was achieved, in some cases maybe no improvements were observed.\n",
        "- We interpreted the confusion matrix results and worked out the precision/recall calculations and compared our findings to the classification_report's.\n",
        "- Finally, we printed the most important features above a certain threshold."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
